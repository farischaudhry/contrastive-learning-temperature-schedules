{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88c4a63",
   "metadata": {},
   "source": [
    "# Global Convergence and Geometry of Contrastive Learning through Temperature Annealing\n",
    "\n",
    "## Mounting Google Drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e963b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ccdbd6",
   "metadata": {},
   "source": [
    "## Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97a494",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c75f64c",
   "metadata": {},
   "source": [
    "### Helper Functions (e.g., normalize, cosine_sim), Data Augmentations, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b0b0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(v):\n",
    "    # Add small epsilon for numerical stability if norms can be zero\n",
    "    return v / (torch.norm(v, dim=-1, keepdim=True) + 1e-9)\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    # a: (N, D), b: (M, D) -> (N, M)\n",
    "    return torch.matmul(a, b.T)\n",
    "\n",
    "def get_cifar10_contrastive_transforms():\n",
    "    # SimCLR-style augmentations for CIFAR-10 (32x32 images)\n",
    "    s = 1.0 # Strength of color jitter\n",
    "    color_jitter = T.ColorJitter(0.8 * s, 0.8 * s, 0.8 * s, 0.2 * s)\n",
    "    transform = T.Compose([\n",
    "        # RandomResizedCrop might be aggressive for 32x32, use RandomCrop + padding.\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomApply([color_jitter], p=0.8),\n",
    "        T.RandomGrayscale(p=0.2),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) # CIFAR-10 stats\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "class ContrastiveCIFAR10Dataset(Dataset):\n",
    "    # Automatically downloads if not present in root\n",
    "    def __init__(self, root='./data', train=True, transform=None):\n",
    "        self.cifar10 = torchvision.datasets.CIFAR10(root=root, train=train, download=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar10)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.cifar10[idx] # Ignore label for contrastive pre-training\n",
    "        if self.transform:\n",
    "            view1 = self.transform(img)\n",
    "            view2 = self.transform(img)\n",
    "            return view1, view2\n",
    "        else:\n",
    "            # Should always have transform for contrastive learning\n",
    "            return img, img\n",
    "\n",
    "# Dataset and Transform for Linear Probe\n",
    "def get_cifar10_linear_probe_transform():\n",
    "     # Only basic normalization needed for evaluation\n",
    "     return T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a658db2",
   "metadata": {},
   "source": [
    "### Model Definition (ResNet + Projection Head) + InfoNCE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveResNet(nn.Module):\n",
    "    # Using ResNet18 as backbone for CIFAR-10\n",
    "    def __init__(self, projection_dim=128, backbone_name='resnet18'):\n",
    "        super().__init__()\n",
    "        if backbone_name == 'resnet18':\n",
    "            #! Can load pre-trained on ImageNet if needed.\n",
    "            # weights = torchvision.models.ResNet18_Weights.IMAGENET1K_V1\n",
    "            # backbone = torchvision.models.resnet18(weights=weights)\n",
    "            backbone = torchvision.models.resnet18(weights=None) # Train from scratch\n",
    "            backbone_dim = backbone.fc.in_features\n",
    "            backbone.fc = nn.Identity() # Remove final classifier layer\n",
    "        #TODO Maybe add other backbones like resnet34, resnet50?\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "        self.backbone = backbone\n",
    "        # MLP Projection Head (SimCLR style: non-linear projection improves representations)\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, backbone_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(backbone_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward_backbone(self, x):\n",
    "        # Get features from the backbone ONLY (for linear probe)\n",
    "        return self.backbone(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Full forward pass for contrastive training\n",
    "        features = self.backbone(x)\n",
    "        projection = self.projection_head(features)\n",
    "        return normalize(projection) # Normalize final output for contrastive loss\n",
    "\n",
    "# InfoNCE Loss for Views\n",
    "def infoNCE_views(view1_embs, view2_embs, beta, device='cpu'):\n",
    "    \"\"\"Calculates InfoNCE loss for two views using logsumexp for stability.\"\"\"\n",
    "    batch_size = view1_embs.shape[0]\n",
    "    embeddings = torch.cat([view1_embs, view2_embs], dim=0) # Shape (2B, D)\n",
    "    embeddings = normalize(embeddings) # Ensure normalized\n",
    "\n",
    "    # Calculate similarity matrix (cosine similarity)\n",
    "    sim_matrix = cosine_sim(embeddings, embeddings) # (2B, 2B)\n",
    "\n",
    "    # --- Calculate Loss using LogSumExp ---\n",
    "    # Loss for anchor i matching positive j: -log[ exp(sim_ij*beta) / sum_{k!=i} exp(sim_ik*beta) ]\n",
    "    # = - ( sim_ij*beta - log[sum_{k!=i} exp(sim_ik*beta)] )\n",
    "    # = log[sum_{k!=i} exp(sim_ik*beta)] - sim_ij*beta\n",
    "\n",
    "    # Scale similarities by beta\n",
    "    scaled_sim_matrix = sim_matrix * beta\n",
    "\n",
    "    # Mask to exclude self-similarity\n",
    "    diag_mask = torch.eye(2 * batch_size, dtype=torch.bool, device=device)\n",
    "    logits_mask = ~diag_mask\n",
    "\n",
    "    # Calculate logsumexp for the denominator term for each row (anchor)\n",
    "    # Need log(sum_{k!=i} exp(beta * s_ik)) = logsumexp(beta * s_ik for k!=i)\n",
    "\n",
    "    # Apply mask before logsumexp to exclude diagonal\n",
    "    # Create matrix where diagonal is -inf (or a very small number) so exp(diag) is zero\n",
    "    large_neg = -torch.finfo(scaled_sim_matrix.dtype).max # A very large negative number\n",
    "    masked_scaled_sim = scaled_sim_matrix.masked_fill(diag_mask, large_neg)\n",
    "\n",
    "    # Now compute logsumexp row-wise\n",
    "    log_denominators = torch.logsumexp(masked_scaled_sim, dim=1) # Shape (2B,)\n",
    "\n",
    "    # Get the scaled positive similarities (numerator terms before log)\n",
    "    scaled_sim_i_iB = torch.diag(scaled_sim_matrix, batch_size) # beta * s_{i, i+B}\n",
    "    scaled_sim_iB_i = torch.diag(scaled_sim_matrix, -batch_size) # beta * s_{i+B, i}\n",
    "\n",
    "    # Calculate loss for pairs (i, i+B) using denominator for anchor i\n",
    "    loss_i = log_denominators[:batch_size] - scaled_sim_i_iB\n",
    "\n",
    "    # Calculate loss for pairs (i+B, i) using denominator for anchor i+B\n",
    "    loss_iB = log_denominators[batch_size:] - scaled_sim_iB_i\n",
    "\n",
    "    # Total loss is the mean over all 2*B pairs\n",
    "    loss = (loss_i.sum() + loss_iB.sum()) / (2 * batch_size)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74763e",
   "metadata": {},
   "source": [
    "### Annealing Schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta(t, total_epochs, schedule_type, beta_low, beta_high, gamma, c_factor): # Added sqrt_c_factor default\n",
    "    \"\"\" Get beta for current epoch t (0-indexed) \"\"\"\n",
    "    epoch_idx = t # Use 0-based index t consistently\n",
    "    total_steps = total_epochs # T\n",
    "\n",
    "    # Ensure beta_low/high are valid\n",
    "    beta_low = max(beta_low, 1e-9) # Ensure positive for ratios/logs if needed\n",
    "    beta_high = max(beta_high, beta_low) # Ensure high >= low\n",
    "\n",
    "    # Handle edge case of zero epochs\n",
    "    if total_steps <= 0: return beta_low\n",
    "\n",
    "    # --- Fixed Schedules ---\n",
    "    if schedule_type == 'fixed_low':\n",
    "        return beta_low\n",
    "    elif schedule_type == 'fixed_high':\n",
    "        return beta_high\n",
    "\n",
    "    # --- Annealing Schedules ---\n",
    "    if schedule_type == 'log':\n",
    "        if total_epochs == 1: return beta_low # Avoid log(2) if only 1 epoch\n",
    "\n",
    "        log_range = np.log(total_steps + 1)\n",
    "        if log_range <= 1e-9: return beta_low # Avoid division by zero/small number if T=0\n",
    "        c = (beta_high - beta_low) / log_range\n",
    "        # Calculate base beta for current epoch\n",
    "        current_beta = beta_low + c * np.log(epoch_idx + 2) # Use log(t+2)\n",
    "        # Apply scaling factor to the change from beta_low\n",
    "        current_beta = beta_low + (current_beta - beta_low) * c_factor\n",
    "        return np.clip(current_beta, beta_low, beta_high)\n",
    "\n",
    "    elif schedule_type == 'linear_tau':\n",
    "        # Linear tau decay from tau_start = 1/beta_low to tau_end = 1/beta_high\n",
    "        tau_start = 1.0 / beta_low\n",
    "        tau_end = 1.0 / beta_high if beta_high > 1e-9 else float('inf')\n",
    "        # Handle infinite tau edge cases safely\n",
    "        if tau_end == float('inf') and tau_start == float('inf'): return beta_low\n",
    "        if tau_end == float('inf'): tau_end = tau_start + 1.0\n",
    "        if tau_start == float('inf'): return beta_low # Cannot start from infinite tau if end is finite\n",
    "        # Interpolate tau using 1-based epoch progress (t+1)/T\n",
    "        progress = (epoch_idx + 1) / total_steps\n",
    "        tau_t = tau_start + (tau_end - tau_start) * progress\n",
    "        return 1.0 / max(tau_t, 1e-9) # Return beta, ensure tau_t doesn't hit zero\n",
    "\n",
    "    elif schedule_type == 'linear_beta':\n",
    "        # 1â€‘based progress\n",
    "        progress = (epoch_idx + 1) / total_steps\n",
    "        # vanilla interpolation\n",
    "        base_beta = beta_low + (beta_high - beta_low) * progress\n",
    "        # scale the change from beta_low by c_factor\n",
    "        scaled  = beta_low + (base_beta - beta_low) * c_factor\n",
    "        return float(np.clip(scaled, beta_low, beta_high))\n",
    "\n",
    "    elif schedule_type == 'sqrt_beta': # Interpolating beta based on sqrt progress\n",
    "         if total_epochs <= 0: return beta_low\n",
    "         # Progress based on sqrt(t+1)/sqrt(T)\n",
    "         progress = np.sqrt(epoch_idx + 1) / np.sqrt(total_steps)\n",
    "         # Apply optional scaling factor\n",
    "         progress = progress * c_factor\n",
    "         # Ensure progress doesn't exceed 1 after scaling\n",
    "         progress = min(progress, 1.0)\n",
    "         current_beta = beta_low + (beta_high - beta_low) * progress\n",
    "         return np.clip(current_beta, beta_low, beta_high)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule type: {schedule_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc11759",
   "metadata": {},
   "source": [
    "### Linear Probe Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04a08c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_embeddings(encoder_backbone, loader, device):\n",
    "    \"\"\"Extract backbone features for all samples in loader.\"\"\"\n",
    "    encoder_backbone.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device)\n",
    "        features = encoder_backbone(images)\n",
    "        all_features.append(features.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "    return torch.cat(all_features), torch.cat(all_labels)\n",
    "\n",
    "def train_linear_probe(encoder_backbone, device, config):\n",
    "    \"\"\"Train and evaluate a linear classifier on frozen features.\"\"\"\n",
    "    print(\"\\n--- Training and Evaluating Linear Probe ---\")\n",
    "\n",
    "    # Get features from the training set using the frozen encoder\n",
    "    train_transform = get_cifar10_linear_probe_transform()\n",
    "    probe_train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=train_transform)\n",
    "    # Use double batch size for faster feature extraction\n",
    "    probe_train_loader = DataLoader(probe_train_dataset, batch_size=config['batch_size'] * 2, shuffle=False, num_workers=config.get('num_workers', 2))\n",
    "    print(\"Extracting training features...\")\n",
    "    X_train, y_train = get_embeddings(encoder_backbone, probe_train_loader, device)\n",
    "    print(f\"Training features shape: {X_train.shape}\")\n",
    "\n",
    "    # 2. Get features from the test set\n",
    "    test_transform = get_cifar10_linear_probe_transform()\n",
    "    probe_test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=test_transform)\n",
    "    probe_test_loader = DataLoader(probe_test_dataset, batch_size=config['batch_size'] * 2, shuffle=False, num_workers=config.get('num_workers', 2))\n",
    "    print(\"Extracting testing features...\")\n",
    "    X_test, y_test = get_embeddings(encoder_backbone, probe_test_loader, device)\n",
    "    print(f\"Testing features shape: {X_test.shape}\")\n",
    "\n",
    "    # Train a linear classifier (Logistic Regression)\n",
    "    print(\"Training logistic regression classifier...\")\n",
    "    classifier = LogisticRegression(random_state=config['seed'], max_iter=1000, C=1.0, solver='liblinear') # Liblinear often good for this\n",
    "    classifier.fit(X_train.numpy(), y_train.numpy())\n",
    "\n",
    "    # 4. Evaluate the classifier\n",
    "    y_pred = classifier.predict(X_test.numpy())\n",
    "    accuracy = accuracy_score(y_test.numpy(), y_pred) * 100\n",
    "    print(f\"Linear Probe Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbbf509",
   "metadata": {},
   "source": [
    "### Main Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_cifar10(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    start_run_time = time.time()\n",
    "\n",
    "    # Extract config vars\n",
    "    seed = config['seed']\n",
    "    schedule = config['schedule']\n",
    "    output_dir = config['output_dir']\n",
    "    no_save = config.get('no_save', False)\n",
    "    epochs = config['epochs']\n",
    "    num_workers = config.get('num_workers', 4) # Default workers\n",
    "\n",
    "    # Setup output dir\n",
    "    run_dir = None\n",
    "    if not no_save:\n",
    "        run_dir = os.path.join(output_dir, f\"schedule_{schedule}_c{config['c_factor']}_seed_{seed}\")\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        with open(os.path.join(run_dir, 'config.json'), 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Data\n",
    "    contrastive_transform = get_cifar10_contrastive_transforms()\n",
    "    contrastive_dataset = ContrastiveCIFAR10Dataset(train=True, transform=contrastive_transform)\n",
    "    contrastive_loader = DataLoader(contrastive_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "    # Model\n",
    "    model = ContrastiveResNet(projection_dim=config['projection_dim']).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=1e-6)\n",
    "    #TODO Learning rate scheduler might be better?\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=0) # Example scheduler\n",
    "\n",
    "    # Training Loop\n",
    "    losses = []\n",
    "    print(f\"\\n--- Training CIFAR-10 with schedule: {schedule} ---\")\n",
    "    epoch_times = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        current_beta = get_beta(epoch, epochs, schedule, config['beta_low'], config['beta_high'], config['exp_gamma'], config['c_factor'])\n",
    "\n",
    "        for view1, view2 in contrastive_loader:\n",
    "            view1, view2 = view1.to(device, non_blocking=True), view2.to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            emb1 = model(view1)\n",
    "            emb2 = model(view2)\n",
    "            loss = infoNCE_views(emb1, emb2, current_beta, device)\n",
    "\n",
    "            # Handle potential NaN loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"Warning: NaN loss detected at epoch {epoch+1}, batch {num_batches+1}. Beta={current_beta}. Skipping update.\")\n",
    "                # For now, just skip optimizer step\n",
    "                continue\n",
    "            else:\n",
    "                 loss.backward()\n",
    "                 optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / max(num_batches, 1)\n",
    "        losses.append(avg_epoch_loss)\n",
    "\n",
    "        # Step the scheduler if using one\n",
    "        # if scheduler: scheduler.step()\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "\n",
    "        # Print progress every 10 epochs or at the end\n",
    "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1 or epoch == 0:\n",
    "             print(f\"Epoch [{epoch+1}/{epochs}], Beta: {current_beta:.2f}, Loss: {avg_epoch_loss:.4f}, Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "\n",
    "    # Final Evaluation: Linear Probe\n",
    "    # Extract the backbone from the trained model\n",
    "    final_encoder_backbone = model.backbone\n",
    "    final_probe_accuracy = train_linear_probe(final_encoder_backbone, device, config)\n",
    "\n",
    "    total_run_time = time.time() - start_run_time\n",
    "    print(f\"Finished Contrastive Training. Total Time: {total_run_time:.2f}s\")\n",
    "\n",
    "    # Save results\n",
    "    results = {\n",
    "        'losses': losses,\n",
    "        'final_probe_accuracy': final_probe_accuracy,\n",
    "        'epoch_times': epoch_times,\n",
    "        'total_training_time': total_run_time\n",
    "    }\n",
    "    if not no_save and run_dir:\n",
    "        # Save only the backbone state_dict if needed for probing later\n",
    "        torch.save(model.backbone.state_dict(), os.path.join(run_dir, 'final_backbone.pth'))\n",
    "        # Or save the full model\n",
    "        torch.save(model.state_dict(), os.path.join(run_dir, 'final_contrastive_model.pth'))\n",
    "        with open(os.path.join(run_dir, 'results.json'), 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\"Results saved to {run_dir}\")\n",
    "\n",
    "        # Plotting\n",
    "        fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Average Contrastive Loss', color=color)\n",
    "        ax1.plot(range(1, epochs + 1), losses, color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax1.grid(True)\n",
    "\n",
    "        # # For plotting beta schedule on secondary axis\n",
    "        # ax2 = ax1.twinx()\n",
    "        # betas = [get_beta(e, epochs, schedule, config['beta_low'], config['beta_high'], config['exp_gamma'], config['log_c_factor']) for e in range(epochs)]\n",
    "        # color = 'tab:blue'\n",
    "        # ax2.set_ylabel('Beta (Inverse Temp)', color=color)\n",
    "        # ax2.plot(range(1, epochs + 1), betas, color=color, linestyle=':')\n",
    "        # ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        plt.title(f\"Loss Curve (Schedule: {schedule}) - Final Probe Acc: {final_probe_accuracy:.2f}%\")\n",
    "        plt.savefig(os.path.join(run_dir, \"contrastive_loss_curve.png\"))\n",
    "        plt.show()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fffda5d",
   "metadata": {},
   "source": [
    "### Config + Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5304dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_config = {\n",
    "    'seed': 1000,\n",
    "    'd_embed': 128,              # Standard embedding dimension for ResNet features\n",
    "    'projection_dim': 128,       # Dimension after projection head\n",
    "    'epochs': 100,               # e.g., 200-400 is common\n",
    "    'batch_size': 256,           # Adjust based on GPU memory (256-512 is common for ResNets)\n",
    "    'lr': 3e-4,                  # Common starting point for Adam with ResNets\n",
    "    'beta_low': 1.0,             # Corresponds to tau=1.0\n",
    "    'beta_high': 10000000.0,     # Corresponds to tau=0.0000001 (Common SimCLR value)\n",
    "    'exp_gamma': 0.95,           # Gamma for exponential TAU decay (adjust if needed)\n",
    "    'c_factor': 1,             # Scaling for schedules\n",
    "    'output_dir': '/content/drive/MyDrive/colab_outputs/cifar10_results', # Output directory for results\n",
    "    'no_save': False,            # Set to True to disable saving models/logs/plots\n",
    "    'num_workers': 2             # Number of workers for DataLoader (adjust based on system)\n",
    "}\n",
    "\n",
    "all_results_cifar = {}\n",
    "# schedules_to_run_cifar = ['fixed_low', 'fixed_high', 'log', 'linear_beta', 'linear_tau', 'sqrt_beta']\n",
    "schedules_to_run_cifar = ['fixed_low', 'fixed_high', 'log', 'sqrt_beta']\n",
    "\n",
    "for sched in schedules_to_run_cifar:\n",
    "    print(f\"\\n{'='*20} RUNNING SCHEDULE: {sched} {'='*20}\")\n",
    "    current_config = cifar_config.copy()\n",
    "    current_config['schedule'] = sched\n",
    "\n",
    "    results = main_cifar10(current_config)\n",
    "    all_results_cifar[sched] = results\n",
    "    print(f\"{'='*20} COMPLETED SCHEDULE: {sched} {'='*20}\")\n",
    "\n",
    "# Aggregate and Compare Results\n",
    "print(\"\\n--- Comparison Across Schedules (CIFAR-10) ---\")\n",
    "print(\"| Schedule     | Final Probe Acc (%) | Final Loss   | Avg Epoch Time (s) |\")\n",
    "print(\"|--------------|---------------------|--------------|--------------------|\")\n",
    "for sched, res in all_results_cifar.items():\n",
    "     if res:\n",
    "         avg_epoch_time = np.mean(res['epoch_times']) if res['epoch_times'] else -1\n",
    "         print(f\"| {sched:<12} | {res['final_probe_accuracy']:<19.2f} | {res['losses'][-1]:<12.4f} | {avg_epoch_time:<18.2f} |\")\n",
    "     else:\n",
    "         print(f\"| {sched:<12} | N/A                 | N/A          | N/A                |\")\n",
    "\n",
    "# Plot comparison of final probe accuracies\n",
    "plt.figure(figsize=(8, 5))\n",
    "schedule_names_cifar = list(all_results_cifar.keys())\n",
    "final_probe_scores = [all_results_cifar[s]['final_probe_accuracy'] if all_results_cifar[s] else 0 for s in schedule_names_cifar]\n",
    "bars = plt.bar(schedule_names_cifar, final_probe_scores)\n",
    "plt.xlabel(\"Annealing Schedu    le\")\n",
    "plt.ylabel(\"Final Linear Probe Accuracy (%)\")\n",
    "plt.title(\"Comparison of Final Probe Accuracy Across Schedules (CIFAR-10)\")\n",
    "plt.grid(axis='y')\n",
    "# Add accuracy values on top of bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, f'{yval:.2f}%', va='bottom', ha='center') # va: vertical alignment\n",
    "\n",
    "if not cifar_config['no_save']:\n",
    "    plt.savefig(os.path.join(cifar_config['output_dir'], \"comparison_probe_accuracy.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
